#!/usr/bin/env python3
"""
Comprehensive test suite for the Automated Transcript Correction & Quality Assurance feature.
Tests all components: API endpoints, UI integration, and correction engine functionality.
"""

import requests
import json
import time
import sys
import os

# Add src directory to path for direct engine testing
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

class TestTranscriptCorrection:
    """Test suite for transcript correction functionality."""
    
    BASE_URL = "http://127.0.0.1:5001"
    
    SAMPLE_TRANSCRIPT = """Hello, welcome to today's meating about artifical inteligence and machine lerning. We're going to discus the latest advancements in AI technolgy and how they effect our bussiness operations.

First, lets talk about natural language procesing. NLP has become incredibley powerfull in recent years, allowing us to analayze large amounts of text data very efectively. However, there are still some chalenges we need to adress.

One major issue is the acuracy of speech recognishion systems. Sometimes they missinterpret words, especialy when dealing with technicle terminology or proper nouns. For example, the system might transcribe "machine learning" as "masheen lerning" or "AI" as "eye".

In conclustion, while AI has made remarkabel progress, we stil need to be carefull about data quality and acuracy. Thank you for your atention, and let's move on to the Q&A sesion."""
    
    def test_correction_engine_direct(self):
        """Test the correction engine directly."""
        print("\nüß™ Testing TranscriptCorrectionEngine directly...")
        
        try:
            from services.transcript_correction import TranscriptCorrectionEngine
            
            engine = TranscriptCorrectionEngine()
            assert engine.correction_available, "Correction engine should be available"
            
            # Test quality analysis
            quality_metrics = engine.analyze_transcript_quality(self.SAMPLE_TRANSCRIPT)
            assert quality_metrics.issues_count > 0, "Should find issues in sample transcript"
            assert 0 <= quality_metrics.overall_score <= 100, "Overall score should be 0-100"
            
            # Test correction suggestions
            suggestions = engine.generate_corrections(self.SAMPLE_TRANSCRIPT)
            assert len(suggestions) > 0, "Should generate correction suggestions"
            
            # Verify some expected corrections
            original_texts = [s.original_text for s in suggestions]
            assert any('meating' in text for text in original_texts), "Should find 'meating' error"
            assert any('artifical' in text for text in original_texts), "Should find 'artifical' error"
            assert any('inteligence' in text for text in original_texts), "Should find 'inteligence' error"
            
            # Test applying corrections
            corrected_text = engine.apply_corrections(self.SAMPLE_TRANSCRIPT, suggestions[:5])
            assert corrected_text != self.SAMPLE_TRANSCRIPT, "Text should be changed after corrections"
            
            # Test session management
            session_id = engine.create_session(self.SAMPLE_TRANSCRIPT, "test_user")
            assert session_id, "Should create session successfully"
            
            session_data = engine.get_session(session_id)
            assert session_data is not None, "Should retrieve session data"
            assert session_data['original_transcript'] == self.SAMPLE_TRANSCRIPT
            
            print("‚úÖ Direct engine test passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå Direct engine test failed: {e}")
            return False
    
    def test_web_interface_accessibility(self):
        """Test that the web interface is accessible."""
        print("\nüåê Testing web interface accessibility...")
        
        try:
            response = requests.get(f"{self.BASE_URL}/transcript-correction", timeout=10)
            
            if response.status_code == 200:
                content = response.text
                
                # Check for essential UI elements
                assert 'Transcript Correction' in content, "Page should have title"
                assert 'Quality Assessment' in content, "Should have quality dashboard"
                assert 'transcript-correction.js' in content, "Should load correction JS"
                assert 'Generate Suggestions' in content, "Should have generate button"
                assert 'Apply All' in content, "Should have apply all button"
                
                print("‚úÖ Web interface accessibility test passed!")
                return True
            elif response.status_code == 302:
                print("‚ö†Ô∏è  Web interface requires authentication (redirected to login)")
                return True  # This is expected behavior
            else:
                print(f"‚ùå Web interface returned status {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Web interface test failed: {e}")
            return False
    
    def test_api_endpoints_with_auth_bypass(self):
        """Test API endpoints (note: may require authentication)."""
        print("\nüîå Testing API endpoints...")
        
        endpoints_to_test = [
            ('/api/correction/dictionaries', 'GET'),
            ('/api/correction/statistics', 'GET')
        ]
        
        for endpoint, method in endpoints_to_test:
            try:
                if method == 'GET':
                    response = requests.get(f"{self.BASE_URL}{endpoint}", timeout=5)
                else:
                    response = requests.post(f"{self.BASE_URL}{endpoint}", 
                                           json={'test': 'data'}, timeout=5)
                
                if response.status_code == 200:
                    print(f"‚úÖ {endpoint} - Success")
                elif response.status_code == 302:
                    print(f"‚ö†Ô∏è  {endpoint} - Requires authentication")
                else:
                    print(f"‚ö†Ô∏è  {endpoint} - Status {response.status_code}")
                    
            except Exception as e:
                print(f"‚ùå {endpoint} - Error: {e}")
        
        print("‚úÖ API endpoint test completed!")
        return True
    
    def test_dependencies_and_models(self):
        """Test that all required dependencies and models are available."""
        print("\nüì¶ Testing dependencies and models...")
        
        dependencies = [
            ('textblob', 'TextBlob NLP'),
            ('spacy', 'spaCy NLP'),
            ('language_tool_python', 'LanguageTool')
        ]
        
        all_available = True
        
        for module, description in dependencies:
            try:
                __import__(module)
                print(f"‚úÖ {description}")
            except ImportError:
                print(f"‚ùå {description} - Not available")
                all_available = False
        
        # Test spaCy model
        try:
            import spacy
            nlp = spacy.load('en_core_web_sm')
            print("‚úÖ spaCy English model")
        except Exception as e:
            print(f"‚ùå spaCy English model - {e}")
            all_available = False
        
        return all_available
    
    def test_correction_accuracy(self):
        """Test the accuracy of corrections on known errors."""
        print("\nüéØ Testing correction accuracy...")
        
        try:
            from services.transcript_correction import TranscriptCorrectionEngine
            
            engine = TranscriptCorrectionEngine()
            if not engine.correction_available:
                print("‚ö†Ô∏è  Correction engine not available - skipping accuracy test")
                return True
            
            # Test cases with known errors and expected corrections
            test_cases = [
                ("This is a meating about AI", "meeting"),
                ("I have artifical inteligence", "artificial"),
                ("We need to discus this", "discuss"),
                ("The acuracy is important", "accuracy"),
                ("Lets talk about NLP", "Let's")
            ]
            
            accuracy_count = 0
            total_tests = len(test_cases)
            
            for text, expected_word in test_cases:
                suggestions = engine.generate_corrections(text)
                
                # Check if any suggestion contains the expected correction
                found_correction = False
                for suggestion in suggestions:
                    if expected_word.lower() in suggestion.suggested_text.lower():
                        found_correction = True
                        accuracy_count += 1
                        break
                
                status = "‚úÖ" if found_correction else "‚ùå"
                print(f"{status} '{text}' -> expected '{expected_word}' - {'Found' if found_correction else 'Not found'}")
            
            accuracy = (accuracy_count / total_tests) * 100
            print(f"\nüìä Correction Accuracy: {accuracy:.1f}% ({accuracy_count}/{total_tests})")
            
            return accuracy >= 60  # Accept 60% accuracy as passing
            
        except Exception as e:
            print(f"‚ùå Accuracy test failed: {e}")
            return False
    
    def test_performance(self):
        """Test performance of correction operations."""
        print("\n‚ö° Testing performance...")
        
        try:
            from services.transcript_correction import TranscriptCorrectionEngine
            
            engine = TranscriptCorrectionEngine()
            if not engine.correction_available:
                print("‚ö†Ô∏è  Correction engine not available - skipping performance test")
                return True
            
            # Test with different text lengths
            test_texts = [
                ("Short text with eror.", "Short"),
                (self.SAMPLE_TRANSCRIPT, "Medium"),
                (self.SAMPLE_TRANSCRIPT * 3, "Long")
            ]
            
            performance_results = []
            
            for text, label in test_texts:
                start_time = time.time()
                
                # Quality analysis
                qa_start = time.time()
                quality_metrics = engine.analyze_transcript_quality(text)
                qa_time = time.time() - qa_start
                
                # Generate suggestions
                sg_start = time.time()
                suggestions = engine.generate_corrections(text)
                sg_time = time.time() - sg_start
                
                total_time = time.time() - start_time
                
                print(f"{label} text ({len(text)} chars):")
                print(f"  Quality analysis: {qa_time:.2f}s")
                print(f"  Generate suggestions: {sg_time:.2f}s")
                print(f"  Total time: {total_time:.2f}s")
                print(f"  Found {len(suggestions)} suggestions")
                
                performance_results.append({
                    'label': label,
                    'length': len(text),
                    'total_time': total_time,
                    'suggestions': len(suggestions)
                })
            
            # Check if performance is reasonable (under 30 seconds for long text)
            long_test = next(r for r in performance_results if r['label'] == 'Long')
            performance_ok = long_test['total_time'] < 30
            
            status = "‚úÖ" if performance_ok else "‚ö†Ô∏è"
            print(f"\n{status} Performance test completed!")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Performance test failed: {e}")
            return False
    
    def run_all_tests(self):
        """Run all tests and provide summary."""
        print("üöÄ Starting Comprehensive Transcript Correction Tests")
        print("=" * 80)
        
        tests = [
            ("Dependencies", self.test_dependencies_and_models),
            ("Direct Engine", self.test_correction_engine_direct),
            ("Web Interface", self.test_web_interface_accessibility),
            ("API Endpoints", self.test_api_endpoints_with_auth_bypass),
            ("Correction Accuracy", self.test_correction_accuracy),
            ("Performance", self.test_performance)
        ]
        
        results = []
        
        for test_name, test_func in tests:
            print(f"\n{'='*20} {test_name} Test {'='*20}")
            try:
                result = test_func()
                results.append((test_name, result))
            except Exception as e:
                print(f"‚ùå {test_name} test crashed: {e}")
                results.append((test_name, False))
        
        # Summary
        print("\n" + "="*80)
        print("üìä TEST SUMMARY")
        print("="*80)
        
        passed = 0
        total = len(results)
        
        for test_name, result in results:
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"{status:8} | {test_name}")
            if result:
                passed += 1
        
        print("-" * 80)
        print(f"Results: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
        
        if passed == total:
            print("üéâ All tests passed! Transcript correction feature is working perfectly!")
        elif passed >= total * 0.8:
            print("‚úÖ Most tests passed! Feature is functional with minor issues.")
        else:
            print("‚ö†Ô∏è  Some tests failed. Review the issues above.")
        
        print("\nüí° Next steps:")
        print("- Open http://127.0.0.1:5001/transcript-correction to test the web interface")
        print("- Try the sample transcript or paste your own text")
        print("- Test different industry dictionaries")
        print("- Export corrected transcripts in various formats")
        
        return passed == total

if __name__ == "__main__":
    print("üß™ Transcript Correction Comprehensive Test Suite")
    print("Make sure the application is running on http://127.0.0.1:5001")
    time.sleep(1)
    
    tester = TestTranscriptCorrection()
    success = tester.run_all_tests()
    
    exit_code = 0 if success else 1
    exit(exit_code)
